{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PATH = \"../../data/raw/\"\n",
    "PROCESSED_PATH = \"../../data/processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(value, filename):\n",
    "    if value is not None and filename is not None:\n",
    "        joblib.dump(value=value, filename=filename)\n",
    "    else:\n",
    "        raise Exception(\"Value or filename cannot be None\".capitalize())\n",
    "    \n",
    "def load(filename):\n",
    "    if filename is not None:\n",
    "        return joblib.load(filename)\n",
    "    else:\n",
    "        raise Exception(\"Filename cannot be None\".capitalize())\n",
    "    \n",
    "def device_init(device = \"mps\"):\n",
    "    if device == \"mps\":\n",
    "        return torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    elif device == \"cuda\":\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader(Dataset):\n",
    "    def __init__(self, image_path = None, image_size = 128, batch_size = 4, split_ratio = 0.30):\n",
    "        self.image_path = image_path\n",
    "        self.batch_size = batch_size\n",
    "        self.split_ratio = split_ratio\n",
    "        self.image_size = image_size\n",
    "        self.channels = 1\n",
    "        self.images = list()\n",
    "        self.masks = list()\n",
    "\n",
    "    def unzip_folder(self):\n",
    "        if os.path.exists(RAW_PATH):\n",
    "            with zipfile.ZipFile(self.image_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(os.path.join(RAW_PATH, \"segmented\"))\n",
    "        else:\n",
    "            raise Exception(\"Raw data folder does not exist\".capitalize())\n",
    "    def base_transformation(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    def mask_transformation(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,)),\n",
    "            transforms.Grayscale(num_output_channels=self.channels)\n",
    "        ])\n",
    "\n",
    "    def split_dataset(self, **kwargs):\n",
    "        images = kwargs[\"images\"]\n",
    "        masks = kwargs[\"masks\"]\n",
    "\n",
    "        return train_test_split(images, masks, test_size=self.split_ratio, random_state=42)\n",
    "\n",
    "    def create_dataloader(self):\n",
    "        images = os.listdir(os.path.join(RAW_PATH, \"segmented\"))[0]\n",
    "        masks = os.listdir(os.path.join(RAW_PATH, \"segmented\"))[1]\n",
    "\n",
    "        try:\n",
    "            images = os.path.join(RAW_PATH, \"segmented\", images)\n",
    "            masks = os.path.join(RAW_PATH, \"segmented\", masks)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        for image in os.listdir(images):\n",
    "            if image not in os.listdir(masks):\n",
    "                continue\n",
    "            else:\n",
    "                image_path = os.path.join(images, image)\n",
    "                mask_path = os.path.join(masks, image)\n",
    "\n",
    "                self.images.append(\n",
    "                    self.base_transformation()(Image.fromarray(cv2.imread(image_path))))\n",
    "                self.masks.append(\n",
    "                    self.mask_transformation()(Image.fromarray(cv2.imread(mask_path))))\n",
    "\n",
    "        image_split = self.split_dataset(images=self.images, masks=self.masks)\n",
    "\n",
    "        if os.path.exists(PROCESSED_PATH):\n",
    "\n",
    "            dataloader = DataLoader(\n",
    "                dataset=list(zip(self.images, self.masks)),\n",
    "                batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "            train_dataloader = DataLoader(\n",
    "                dataset=list(zip(image_split[0], image_split[2])),\n",
    "                batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "            test_dataloader = DataLoader(\n",
    "                dataset=list(zip(image_split[1], image_split[3])),\n",
    "                batch_size=self.batch_size*4,\n",
    "                shuffle=True,\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                dump(\n",
    "                    value=dataloader, filename=os.path.join(PROCESSED_PATH, \"dataloader.pkl\"))\n",
    "\n",
    "                dump(\n",
    "                    value=train_dataloader, filename=os.path.join(PROCESSED_PATH, \"train_dataloader.pkl\"))\n",
    "\n",
    "                dump(\n",
    "                    value=test_dataloader, filename=os.path.join(PROCESSED_PATH, \"test_dataloader.pkl\"))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            raise Exception(\"Processed data folder does not exist\".capitalize())\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "    @staticmethod\n",
    "    def details_dataset():\n",
    "        if os.path.exists(PROCESSED_PATH):\n",
    "            \n",
    "            dataloader = load(os.path.join(PROCESSED_PATH, \"dataloader.pkl\"))\n",
    "            images, masks = next(iter(dataloader))\n",
    "            print(\n",
    "                \"Total number of the images in the dataset is {}\".format(\n",
    "                    sum(image.size(0) for image, _ in dataloader)\n",
    "                )\n",
    "            )\n",
    "            print(\n",
    "                \"Total number of the masks in the dataset is {}\\n\\n\".format(\n",
    "                    sum(masks.size(0) for _, masks in dataloader)\n",
    "                )\n",
    "            )\n",
    "            print(\"The shape of the images is {}\\nThe shape of the masks is {}\".format(images.size(), masks.size()))\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Processed data folder does not exist\".capitalize())\n",
    "\n",
    "    @staticmethod\n",
    "    def data_normalized(**kwargs):\n",
    "        return (kwargs[\"data\"] - kwargs[\"data\"].min()) / (kwargs[\"data\"].max() - kwargs[\"data\"].min())\n",
    "\n",
    "    @staticmethod\n",
    "    def display_images():\n",
    "        if os.path.exists(PROCESSED_PATH):\n",
    "            dataloader = load(os.path.join(PROCESSED_PATH, \"test_dataloader.pkl\"))\n",
    "            images, masks = next(iter(dataloader))\n",
    "\n",
    "            plt.figure(figsize=(30, 15))\n",
    "\n",
    "            for index, image in enumerate(images):\n",
    "                image = image.permute(1, 2, 0)\n",
    "                mask = masks[index].permute(1, 2, 0)\n",
    "\n",
    "                image = Loader.data_normalized(data=image)\n",
    "                mask = Loader.data_normalized(data=mask)\n",
    "\n",
    "                plt.subplot(2 * 4, 2 * 4, 2 * index + 1)\n",
    "                plt.imshow(image)\n",
    "                plt.title(\"Image\")\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                plt.subplot(2 * 4, 2 * 4, 2 * index + 2)\n",
    "                plt.imshow(mask)\n",
    "                plt.title(\"Mask\")\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Processed data folder does not exist\".capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader(image_path=\"/Users/shahmuhammadraditrahman/Desktop/brain.zip\", batch_size=4, image_size=128, split_ratio=0.30)\n",
    "loader.unzip_folder()\n",
    "dataloader = loader.create_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loader.details_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loader.display_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, train_label = next(iter(train_dataloader))\n",
    "# test_data, test_label = next(iter(test_dataloader))\n",
    "\n",
    "\n",
    "# train_data.size(), train_label.size(), test_data.size(), test_label.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=None, out_channels=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.model = self.encoder_block()\n",
    "\n",
    "    def encoder_block(self):\n",
    "        layers = OrderedDict()\n",
    "        layers[\"conv1\"] = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        layers[\"relu1\"] = nn.ReLU(inplace=True)\n",
    "        layers[\"conv2\"] = nn.Conv2d(\n",
    "            in_channels=self.out_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        layers[\"batch_norm1\"] = nn.BatchNorm2d(self.out_channels)\n",
    "        layers[\"relu2\"] = nn.ReLU(inplace=True)\n",
    "\n",
    "        return nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x) if x is not None else None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    encoder = Encoder(in_channels=3, out_channels=64)\n",
    "    assert encoder(torch.randn(64, 3, 128, 128)).shape == (64, 64, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels=None, out_channels=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.model = self.decoder_block()\n",
    "\n",
    "    def decoder_block(self):\n",
    "        layers = OrderedDict()\n",
    "        layers[\"deconv1\"] = nn.ConvTranspose2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "        )\n",
    "        return nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, x=None, skip_info=None):\n",
    "        if x is not None and skip_info is not None:\n",
    "            return torch.cat((self.model(x), skip_info), dim=1)\n",
    "        elif x is not None and skip_info is None:\n",
    "            return self.model(x)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    encoder = Encoder(in_channels=3, out_channels=64)\n",
    "    decoder = Decoder(in_channels=64, out_channels=64)\n",
    "\n",
    "    skip_info = encoder(torch.randn(64, 3, 256, 256))\n",
    "    noise_samples = torch.randn(64, 64, 128, 128)\n",
    "\n",
    "    assert decoder(noise_samples, skip_info).shape == (64, 128, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder_layer1 = Encoder(in_channels=3, out_channels=64)\n",
    "        self.encoder_layer2 = Encoder(in_channels=64, out_channels=128)\n",
    "        self.encoder_layer3 = Encoder(in_channels=128, out_channels=256)\n",
    "        self.encoder_layer4 = Encoder(in_channels=256, out_channels=512)\n",
    "        self.bottom_layer = Encoder(in_channels=512, out_channels=1024)\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.intermediate_layer1 = Encoder(in_channels=1024, out_channels=512)\n",
    "        self.intermediate_layer2 = Encoder(in_channels=512, out_channels=256)\n",
    "        self.intermediate_layer3 = Encoder(in_channels=256, out_channels=128)\n",
    "        self.intermediate_layer4 = Encoder(in_channels=128, out_channels=64)\n",
    "\n",
    "        self.decoder_layer1 = Decoder(in_channels=1024, out_channels=512)\n",
    "        self.decoder_layer2 = Decoder(in_channels=512, out_channels=256)\n",
    "        self.decoder_layer3 = Decoder(in_channels=256, out_channels=128)\n",
    "        self.decoder_layer4 = Decoder(in_channels=128, out_channels=64)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        enc1_out = self.encoder_layer1(x)\n",
    "        pooled_enc1 = self.max_pool(enc1_out)\n",
    "\n",
    "        enc2_out = self.encoder_layer2(pooled_enc1)\n",
    "        pooled_enc2 = self.max_pool(enc2_out)\n",
    "\n",
    "        enc3_out = self.encoder_layer3(pooled_enc2)\n",
    "        pooled_enc3 = self.max_pool(enc3_out)\n",
    "\n",
    "        enc4_out = self.encoder_layer4(pooled_enc3)\n",
    "        pooled_enc4 = self.max_pool(enc4_out)\n",
    "\n",
    "        bottom_out = self.bottom_layer(pooled_enc4)\n",
    "\n",
    "        # Decoder path\n",
    "        dec1_input = self.decoder_layer1(bottom_out, enc4_out)\n",
    "        dec1_out = self.intermediate_layer1(dec1_input)\n",
    "\n",
    "        dec2_input = self.decoder_layer2(dec1_out, enc3_out)\n",
    "        dec2_out = self.intermediate_layer2(dec2_input)\n",
    "\n",
    "        dec3_input = self.decoder_layer3(dec2_out, enc2_out)\n",
    "        dec3_out = self.intermediate_layer3(dec3_input)\n",
    "\n",
    "        dec4_input = self.decoder_layer4(dec3_out, enc1_out)\n",
    "        dec4_out = self.intermediate_layer4(dec4_input)\n",
    "\n",
    "        # Final output\n",
    "        final_output = self.final_layer(dec4_out)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AttentionNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels=None, out_channels=None):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.W_gate = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=self.out_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=True,\n",
    "            ),\n",
    "            nn.BatchNorm2d(self.out_channels),\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=self.out_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=True,\n",
    "            ),\n",
    "            nn.BatchNorm2d(self.out_channels),\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.in_channels * 2,\n",
    "                out_channels=1,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=True,\n",
    "            ),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "    def forward(self, x, skip_info):\n",
    "        transformed_input = self.W_gate(x)\n",
    "\n",
    "        if skip_info is not None:\n",
    "            transformed_skip = self.W_x(skip_info)\n",
    "\n",
    "        merged_features = self.relu(torch.cat((transformed_input, transformed_skip), dim=1))\n",
    "        attention_weights = self.psi(merged_features)\n",
    "\n",
    "        return transformed_skip * attention_weights\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    out_result = torch.randn(64, 512, 16, 16)\n",
    "    skip_info = torch.randn(64, 512, 16, 16)\n",
    "    \n",
    "    attention = AttentionBlock(in_channels=512, out_channels=512)\n",
    "    assert attention(out_result, skip_info).shape == (64, 512, 16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionUNet, self).__init__()\n",
    "\n",
    "        self.encoder_layer1 = Encoder(in_channels=3, out_channels=64)\n",
    "        self.encoder_layer2 = Encoder(in_channels=64, out_channels=128)\n",
    "        self.encoder_layer3 = Encoder(in_channels=128, out_channels=256)\n",
    "        self.encoder_layer4 = Encoder(in_channels=256, out_channels=512)\n",
    "        self.bottom_layer = Encoder(in_channels=512, out_channels=1024)\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.intermediate_layer1 = Encoder(in_channels=1024, out_channels=512)\n",
    "        self.intermediate_layer2 = Encoder(in_channels=512, out_channels=256)\n",
    "        self.intermediate_layer3 = Encoder(in_channels=256, out_channels=128)\n",
    "        self.intermediate_layer4 = Encoder(in_channels=128, out_channels=64)\n",
    "\n",
    "        self.decoder_layer1 = Decoder(in_channels=1024, out_channels=512)\n",
    "        self.decoder_layer2 = Decoder(in_channels=512, out_channels=256)\n",
    "        self.decoder_layer3 = Decoder(in_channels=256, out_channels=128)\n",
    "        self.decoder_layer4 = Decoder(in_channels=128, out_channels=64)\n",
    "\n",
    "        self.attention_layer1 = AttentionBlock(in_channels=512, out_channels=512)\n",
    "        self.attention_layer2 = AttentionBlock(in_channels=256, out_channels=256)\n",
    "        self.attention_layer3 = AttentionBlock(in_channels=128, out_channels=128)\n",
    "        self.attention_layer4 = AttentionBlock(in_channels=64, out_channels=64)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder layers\n",
    "        encoder_1_output = self.encoder_layer1(x)\n",
    "        pooled_encoder_1_output = self.max_pool(encoder_1_output)\n",
    "\n",
    "        encoder_2_output = self.encoder_layer2(pooled_encoder_1_output)\n",
    "        pooled_encoder_2_output = self.max_pool(encoder_2_output)\n",
    "\n",
    "        encoder_3_output = self.encoder_layer3(pooled_encoder_2_output)\n",
    "        pooled_encoder_3_output = self.max_pool(encoder_3_output)\n",
    "\n",
    "        encoder_4_output = self.encoder_layer4(\n",
    "            pooled_encoder_3_output\n",
    "        )\n",
    "        pooled_encoder_4_output = self.max_pool(encoder_4_output)\n",
    "\n",
    "        bottom_layer_output = self.bottom_layer(\n",
    "            pooled_encoder_4_output\n",
    "        )\n",
    "        decoder_1_input = self.decoder_layer1(bottom_layer_output)\n",
    "\n",
    "        attention_1_output = self.attention_layer1(decoder_1_input, encoder_4_output)\n",
    "        merged_attention_1_output = torch.cat((attention_1_output, encoder_4_output), dim=1)\n",
    "        decoder_1_output = self.intermediate_layer1(merged_attention_1_output)\n",
    "\n",
    "        decoder_2_input = self.decoder_layer2(decoder_1_output)\n",
    "        attention_2_output = self.attention_layer2(decoder_2_input, encoder_3_output)\n",
    "        merged_attention_2_output = torch.cat((attention_2_output, encoder_3_output), dim=1)\n",
    "        decoder_2_output = self.intermediate_layer2(merged_attention_2_output)\n",
    "\n",
    "        decoder_3_input = self.decoder_layer3(decoder_2_output)\n",
    "        attention_3_output = self.attention_layer3(decoder_3_input, encoder_2_output)\n",
    "        merged_attention_3_output = torch.cat((attention_3_output, encoder_2_output), dim=1)\n",
    "        decoder_3_output = self.intermediate_layer3(merged_attention_3_output)\n",
    "\n",
    "        decoder_4_input = self.decoder_layer4(decoder_3_output)\n",
    "        attention_4_output = self.attention_layer4(decoder_4_input, encoder_1_output)\n",
    "        merged_attention_4_output = torch.cat((attention_4_output, encoder_1_output), dim=1)\n",
    "        decoder_4_output = self.intermediate_layer4(merged_attention_4_output)\n",
    "\n",
    "        # Final layer\n",
    "        final_output = self.final_layer(decoder_4_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        \n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, predicted, actual):\n",
    "        predicted = predicted.contiguous().view(-1)\n",
    "        actual = actual.contiguous().view(-1)\n",
    "        \n",
    "        intersection = (predicted * actual).sum()\n",
    "        \n",
    "        dice =  (2.0 * intersection + self.smooth) / (predicted.sum() + actual.sum() + self.smooth)\n",
    "        \n",
    "        return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaccardLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(JaccardLoss, self).__init__()\n",
    "        \n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, predicted, actual):\n",
    "        predicted = predicted.contiguous().view(-1)\n",
    "        actual = actual.contiguous().view(-1)\n",
    "        \n",
    "        intersection = (predicted * actual).sum()\n",
    "        union = (predicted + actual).sum() - intersection\n",
    "        \n",
    "        return (1.0 - intersection + self.smooth) / (union + self.smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=0.25):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, predicted, actual):\n",
    "        predicted = predicted.contiguous().view(-1)\n",
    "        actual = actual.contiguous().view(-1)\n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        BCE = criterion(predicted, actual)\n",
    "        pt = torch.exp(-BCE)\n",
    "        \n",
    "        return self.alpha * (1 - pt) ** self.gamma * BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComboLoss(nn.Module):\n",
    "    def __init__(self, smooth = 1e-6, alpha=0.25, gamma=2):\n",
    "        super(ComboLoss, self).__init__()\n",
    "        \n",
    "        self.smooth = smooth\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, predicted, actual):\n",
    "        predicted = predicted.contiguous().view(-1)\n",
    "        actual = actual.contiguous().view(-1)\n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        BCE = criterion(predicted, actual)\n",
    "        \n",
    "        pt = torch.exp(-BCE)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE\n",
    "        \n",
    "        intersection = (predicted * actual).sum()\n",
    "        \n",
    "        dice_loss = 1 - (2.0 * intersection + self.smooth) / (predicted.sum() + actual.sum() + self.smooth)\n",
    "        \n",
    "        return (dice_loss + focal_loss) + BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helpers(**kwargs):\n",
    "    is_attentionUNet = kwargs[\"is_attentionUNet\"]\n",
    "    device = device=kwargs[\"device\"]\n",
    "    loss = kwargs[\"loss\"]\n",
    "\n",
    "    \n",
    "    if is_attentionUNet == True:\n",
    "        model = AttentionUNet().to(device)\n",
    "    else:\n",
    "        model = UNet().to(device)\n",
    "        \n",
    "    optimizer = optim.Adam(model.parameters(), lr=kwargs[\"lr\"], betas=(0.5, 0.999))\n",
    "    \n",
    "    if os.path.exists(PROCESSED_PATH):\n",
    "        train_dataloader = load(filename=os.path.join(PROCESSED_PATH, \"train_dataloader.pkl\"))\n",
    "        test_dataloader = load(filename=os.path.join(PROCESSED_PATH, \"test_dataloader.pkl\"))\n",
    "    else:\n",
    "        raise Exception(\"Dataloader - train & test cannot be loaded\".capitalize())\n",
    "    \n",
    "    if loss == \"dice\":\n",
    "        criterion = DiceLoss(smooth=kwargs[\"smooth\"])\n",
    "    elif loss == \"dice_bce\":\n",
    "        criterion = DiceLoss(smooth=kwargs[\"smooth\"])\n",
    "    elif loss == \"IoU\":\n",
    "        criterion = JaccardLoss(smooth=kwargs[\"smooth\"])\n",
    "    elif loss == \"focal\":\n",
    "        criterion = FocalLoss(gamma=kwargs[\"gamma\"], alpha=kwargs[\"alpha\"])\n",
    "    elif loss == \"combo\":\n",
    "        criterion = ComboLoss(smooth=kwargs[\"smooth\"], alpha=kwargs[\"alpha\"], gamma=kwargs[\"gamma\"])\n",
    "    else:\n",
    "        criterion = nn.BCELoss()\n",
    "    \n",
    "    return {\"model\": model,\n",
    "            \"optimizer\": optimizer,\n",
    "            \"criterion\": criterion,\n",
    "            \"train_dataloader\": train_dataloader,\n",
    "            \"test_dataloader\": test_dataloader\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, epochs = 50,\n",
    "        lr = 1e-2,\n",
    "        loss = None,\n",
    "        is_attentionUNet = False,\n",
    "        smooth = 0.01,\n",
    "        alpha = 0.25,\n",
    "        gamma = 2,\n",
    "        beta1 = 0.9,\n",
    "        beta2 = 0.999,\n",
    "        device = \"mps\",\n",
    "        display = True\n",
    "        ):\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.loss = loss\n",
    "        self.is_attentionUNet = is_attentionUNet\n",
    "        self.smooth = smooth\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.device = device_init(device)\n",
    "        self.display = display\n",
    "\n",
    "        self.setup = helpers(\n",
    "                is_attentionUNet = self.is_attentionUNet,\n",
    "                device = self.device,\n",
    "                lr = self.lr,\n",
    "                loss = self.loss,\n",
    "                smooth = self.smooth,\n",
    "                alpha = self.alpha,\n",
    "                gamma = self.gamma)\n",
    "\n",
    "        self.model = self.setup[\"model\"]\n",
    "        self.optimizer = self.setup[\"optimizer\"]\n",
    "        self.criterion = self.setup[\"criterion\"]\n",
    "        self.train_dataloader = self.setup[\"train_dataloader\"]\n",
    "        self.test_dataloader = self.setup[\"test_dataloader\"]\n",
    "\n",
    "    def update_train(self, **kwargs):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        train_predicted_masks = self.model(kwargs[\"images\"])\n",
    "\n",
    "        train_predicted_loss = self.criterion(train_predicted_masks, kwargs[\"masks\"])\n",
    "\n",
    "        train_predicted_loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return train_predicted_loss.item()\n",
    "\n",
    "    def update_test(self, **kwargs):\n",
    "        test_predicted_masks = self.model(kwargs[\"images\"])\n",
    "\n",
    "        test_predicted_loss = self.criterion(test_predicted_masks, kwargs[\"masks\"])\n",
    "\n",
    "        return test_predicted_loss.item()\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            total_train_loss = list()\n",
    "            total_test_loss = list()\n",
    "\n",
    "            for images, masks in self.train_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                masks = masks.to(self.device)\n",
    "\n",
    "                total_train_loss.append(self.update_train(images=images, masks=masks))\n",
    "\n",
    "            for images, masks in self.test_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                masks = masks.to(self.device)\n",
    "\n",
    "                total_test_loss.append(self.update_test(images=images, masks=masks))\n",
    "\n",
    "            print(\"Epoch - [{}/{}] - train_loss: {:.5f} - test_loss: {:.5f}\".format(\n",
    "                epoch+1, self.epochs, np.mean(total_train_loss), np.mean(total_test_loss)))\n",
    "\n",
    "            image, masks = next(iter(self.train_dataloader))\n",
    "            image = image.to(self.device)\n",
    "            masks = masks.to(self.device)\n",
    "\n",
    "            pred_masks = self.model(image)\n",
    "            save_image(\n",
    "                pred_masks,\n",
    "                os.path.join(\n",
    "                    \"../../outputs/train_images/image_{}.png\".format(epoch + 1)\n",
    "                ),\n",
    "                normalize=True,\n",
    "            )\n",
    "\n",
    "        torch.save(self.model.state_dict(), \"../../checkpoints/best_model/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(epochs=15, lr=1e-4, loss=\"tversky\", smooth=0.001, device=\"mps\", display=True)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "model = UNet().to(device)\n",
    "model.load_state_dict(torch.load(\"../../checkpoints/best_model/model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test:\n",
    "    def __init__(self, device = \"mps\"):\n",
    "        self.device = device_init(device)\n",
    "\n",
    "    def plot(self):        \n",
    "        if os.path.exists(PROCESSED_PATH):\n",
    "            dataloader = load(filename=os.path.join(PROCESSED_PATH, \"test_dataloader.pkl\"))\n",
    "        else:\n",
    "            raise Exception(\"Dataloader - test cannot be loaded\".capitalize())\n",
    "\n",
    "        images, masks = next(iter(dataloader))\n",
    "        pred_masks = model(images.to(self.device))\n",
    "\n",
    "        plt.figure(figsize=(30, 15))\n",
    "\n",
    "        for index, image in enumerate(pred_masks):\n",
    "            image = image.permute(1, 2, 0)\n",
    "            image = image.cpu().detach().numpy()\n",
    "            image = (image - image.min())/(image.max() - image.min())\n",
    "\n",
    "            plt.subplot(2*4, 2*4, 2*index+1)\n",
    "            plt.imshow(image, cmap=\"gray\")\n",
    "            plt.title(\"Image\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            mask = masks[index].permute(1, 2, 0)\n",
    "            mask = mask.cpu().detach().numpy()\n",
    "            mask = (mask - mask.min())/(mask.max() - mask.min())\n",
    "\n",
    "            plt.subplot(2 * 4, 2 * 4, 2 * index + 2)\n",
    "            plt.imshow(mask, cmap=\"gray\")\n",
    "            plt.title(\"Ground Truth\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Test(device=\"mps\")\n",
    "test.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AttentionUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    epochs=15, lr=1e-4, loss=\"tversky\", is_attentionUNet=True, smooth=0.001, device=\"mps\", display=True\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "model = AttentionUNet().to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"../../checkpoints/best_model/model.pth\"))\n",
    "test.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
